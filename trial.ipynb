{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bd23961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from enum import auto\n",
    "from enum import Enum\n",
    "from itertools import combinations\n",
    "from pprint import pprint\n",
    "from unicodedata import normalize\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9918a257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to .nltk/...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\", download_dir='.nltk/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7550e857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading datasets done.\n"
     ]
    }
   ],
   "source": [
    "print('Loading datasets...')\n",
    "dataset_snli = load_dataset('stanfordnlp/snli', split='test')\n",
    "dataset_mnli = load_dataset('nyu-mll/multi_nli', split='validation_matched')\n",
    "dataset_anli = load_dataset('facebook/anli', split='test_r3')\n",
    "print('Loading datasets done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e8e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label(Enum):\n",
    "    ENTAILMENT = auto()\n",
    "    NOT_ENTAILMENT = auto()\n",
    "    NOT_AVAILABLE = auto()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Example:\n",
    "    premise: str\n",
    "    hypothesis: str\n",
    "    label: Label\n",
    "\n",
    "\n",
    "class ExampleSNLI(Example):\n",
    "    def __init__(self, example: dict):\n",
    "        label = self._to_binary_class(example['label'])\n",
    "        super().__init__(example['premise'], example['hypothesis'], label)\n",
    "    \n",
    "    def _to_binary_class(self, label: int) -> Label:\n",
    "        return Label.ENTAILMENT if label == 0 else Label.NOT_ENTAILMENT\n",
    "\n",
    "\n",
    "class ExampleMNLI(Example):\n",
    "    def __init__(self, example: dict):\n",
    "        label = self._to_binary_class(example['label'])\n",
    "        super().__init__(example['premise'], example['hypothesis'], label)\n",
    "    \n",
    "    def _to_binary_class(self, label: int) -> Label:\n",
    "        return Label.ENTAILMENT if label == 0 else Label.NOT_ENTAILMENT\n",
    "\n",
    "\n",
    "class ExampleANLI(Example):\n",
    "    def __init__(self, example: dict):\n",
    "        label = self._to_binary_class(example['label'])\n",
    "        super().__init__(example['premise'], example['hypothesis'], label)\n",
    "    \n",
    "    def _to_binary_class(self, label: int) -> Label:\n",
    "        return Label.ENTAILMENT if label == 0 else Label.NOT_ENTAILMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4337fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_snli = [ExampleSNLI(example) for example in dataset_snli]\n",
    "examples_mnli = [ExampleMNLI(example) for example in dataset_mnli]\n",
    "examples_anli = [ExampleANLI(example) for example in dataset_anli]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abe8dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    "    @abstractmethod\n",
    "    def predict(self, example: Example) -> int:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict_batch(self, examples: list[Example]) -> list[int]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class SBERT(Model):\n",
    "    def __init__(self, pretrained_model_name: str):\n",
    "        self.model = CrossEncoder(pretrained_model_name)\n",
    "        self.ENTAILMENT = 1 # see https://huggingface.co/cross-encoder/nli-deberta-v3-small\n",
    "    \n",
    "    def predict(self, example: Example) -> int:\n",
    "        score = self.model.predict((example.premise, example.hypothesis))[0]\n",
    "        return Label.ENTAILMENT if np.argmax(score) == self.ENTAILMENT else Label.NOT_ENTAILMENT\n",
    "    \n",
    "    def predict_batch(self, examples: list[Example]) -> list[int]:\n",
    "        scores = self.model.predict([(example.premise, example.hypothesis) for example in examples])\n",
    "        return [Label.ENTAILMENT if np.argmax(score) == self.ENTAILMENT else Label.NOT_ENTAILMENT for score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26ffe23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_sentences(text: str, k: int) -> list[str]:\n",
    "    sentences = sent_tokenize(normalize(text))\n",
    "    return [' '.join(comb) for comb in combinations(sentences, k)]\n",
    "\n",
    "\n",
    "def sub_example(tokenizer, example: Example, l: int) -> list[Example]:\n",
    "    assert l > 0, 'The number of selected sentences must be greater than 0'\n",
    "    max_length = tokenizer.model_max_length\n",
    "\n",
    "    sub_examples = []\n",
    "    for k in range(1, l + 1):\n",
    "        sub_premises = select_k_sentences(example.premise, k)\n",
    "        for sub_premise in sub_premises:\n",
    "            if len(tokenizer(sub_premise, example.hypothesis)['input_ids']) > max_length:\n",
    "                continue\n",
    "            sub_examples.append(Example(sub_premise, example.hypothesis, example.label))\n",
    "    return sub_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f9805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true: list[Label], y_pred: list[Label]) -> tuple[float, float, float, float]:\n",
    "    y_true = [0 if label == Label.ENTAILMENT else 1 for label in y_true]\n",
    "    y_pred = [0 if label == Label.ENTAILMENT else 1 for label in y_pred]\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate_model_metrics(model: Model, examples: list[Example], use_batch: bool = True) -> dict:\n",
    "    y_true = [example.label for example in examples]\n",
    "    y_pred = model.predict_batch(examples) if use_batch else [model.predict(example) for example in tqdm(examples)]\n",
    "    accuracy, precision, recall, f1 = metrics(y_true, y_pred)\n",
    "    return { 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8a6bdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on SNLI...\n",
      "{'accuracy': 0.9413,\n",
      " 'f1': 0.9554120774781618,\n",
      " 'precision': 0.9626511556712077,\n",
      " 'recall': 0.9482810615199035}\n",
      "Evaluating model on MNLI...\n",
      "{'accuracy': 0.9273560876209883,\n",
      " 'f1': 0.9441878669275929,\n",
      " 'precision': 0.9366361236216804,\n",
      " 'recall': 0.9518623737373737}\n",
      "Evaluating model on ANLI...\n",
      "{'accuracy': 0.6066666666666667,\n",
      " 'f1': 0.7386489479512736,\n",
      " 'precision': 0.6617063492063492,\n",
      " 'recall': 0.8358395989974937}\n"
     ]
    }
   ],
   "source": [
    "model = SBERT('cross-encoder/nli-deberta-v3-small')\n",
    "\n",
    "print('Evaluating model on SNLI...')\n",
    "metrics_snli = evaluate_model_metrics(model, examples_snli)\n",
    "pprint(metrics_snli)\n",
    "\n",
    "print('Evaluating model on MNLI...')\n",
    "metrics_mnli = evaluate_model_metrics(model, examples_mnli)\n",
    "pprint(metrics_mnli)\n",
    "\n",
    "print('Evaluating model on ANLI...')\n",
    "metrics_anli = evaluate_model_metrics(model, examples_anli)\n",
    "pprint(metrics_anli)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
